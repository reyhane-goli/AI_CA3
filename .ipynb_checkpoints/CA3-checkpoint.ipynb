{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "\n",
    "نام پروژه: Naïve bayes classifier\n",
    "\n",
    "\n",
    "هدف پروژه: استفاده از Naïve bayes classifier به منظور تجزیه و تحلیل کامنت ها و تشخیص مثبت و منفی بودن دیدگاه کاربران\n",
    "توضیح کلی پروژه : در این پروژه می خواهیم با استفاده از Naïve bayes classifier به تجزیه و تحلیل کامنت هایی که در فایل comment_test.csv آمده اند بپردازیم و مثبت یا منفی بودن آن ها و یا در واقع recommended  یا  not_recommended  بودن آن ها را مشخص کنیم. به این منظور می بایست در ابتدا با استفاده از اطلاعات فایل comment_train.csv مدل خود را train کنیم و سپس روی داده های فایل test مدل را اجرا کرده و تشخیص لازم را انجام دهیم.\n",
    "\n",
    "\n",
    "توضیح روند پروژه:\n",
    "\n",
    "\n",
    "فاز اول: پیش پردازش داده ها:\n",
    "\n",
    "همان طور که گفته شد در این پروژه فایلی با نام comment_train.csv در اختیار داریم که حاوی نظرات کاربران دیجی کالا می باشد . این فایل حاوی سه ستون title , comment,recommend می باشد که می خواهیم در ابتدا این داده ها را تحلیل کنیم و با توجه به تجزیه و تحلیلی که روی داده ها انجام دادی و با بهره گیری از Naïve bayes classifier مدلی به دست آوریم که از آن مدل در تشخیص recommend بودن یا not_recommend بودن هر یک از سطر های فایل test استفاده کنیم.\n",
    "به این منظور در ابتدا برای آن که روی داده هایمان بتوانیم بهتر و دقیق تر تجزیه و تحلیل کنیم لازم است که آن ها را پیش پردازش کنیم.(هم داده های فایل test و هم داده های فایل train) برای این کار از کتابخانه هضم استفاده می کنیم که کتابخانه ای است که برای عملیات پیش پردازش روی متون فارسی مورد استفاده قرار می گیرد.  با استفاده از normalizer.normalize() می توانیم فاصله ها را تبدیل به نیم فاصله کنیم و برخی موارد را اصلاح کنیم. سپس از خروجی تابع بالا که متنی normalize شده است را به عنوان وردی تابع word_tokenize استفاده می کنیم. این تابع همان طور که از اسمش مشخص است متن را تبدیل به لیستی از توکن ها می کند و این برای تجزیه و تحلیل داده ها بسیار مهم است. در میان این توکن ها ، توکن هایی وجود دارد که بسیار تکرار می شوند و درنتیجه ی تجزیه و تحلیل ما اثری ندارند مانند کلماتی مثل : از، به، در،که و ... این کلمات همان stop words ها می باشند که باید آن ها را حذف کنیم. برای این کار می توان لیستی از stop word ها را در اینترنت پیدا کرد و آن ها را از متن مورد نظر حذف کرد ولی من در اینجا صرفا همه ی توکن هایی که دارای طول کوچک تر یا مساوی دو می باشند غیر از دو کلمه ی \"کم\" و \"بد\" را حذف کردم که این کار خود به خود منجر به حذف علائم نگارشی نیز می شود.\n",
    "پس از حذف stop words ها حالا تمام کلمه های باقی مانده را به کمک تابع stemmer()  ریشه یابی می کنیم.\n",
    "در این بخش از پروژه روی داده های فایل iterate می کنیم و به ازای هر سطر  روی بخش title , comment تمامی مراحل گفته شده را اجرا می کنیم و توکن های نهایی هر سطر را در برای فایل train ، در لیستی به نام train_tokenlist ریخته  و لیست را به دیکشنری مورد نظریعنی همان train_tokendict  اضافه می کنیم . این دیکشنری در انتهای بررسی تمامی سطر های فایل train شامل لیست هایی است که هر لیست حاوی توکن های متناظر با یک سطر است پس در آن مجموعه ای از توکن های فایل train  در بخش های title و comment را داریم. برای داده های فایل test هم همین روند را تکرار می کنیم با این تفاوت که توکن های هر سطر را ابتدا در لیستی به نام test_tokenlist می ریزیم.\n",
    "همان طور که پیش تر اشاراتی شد برای ریشه یابی یک کلمه می توان از دو متد stemming , lemmatization استفاده کرد. در تابع stemmer کلمه ای که به آن داده می شود تنها ریشه یابی می شود بدون آن که به کلمات اطراف آن توجهی شود اما در lemmatization کلمه ی مورد نظر با توجه به دامنه لغاتی که در آن حضور دارد و نقش اش و کلمات اطراف آن ریشه یابی می شود که دقیق تر است. اما باعث می شود زمان بیش تری برای ریشه یابی کلمات نیاز داشته باشیم و کمی کند تر است.\n",
    "مثالی از تفاوت این دو تابع این است که اگر به تابع stemmer کلمه ی \"عالی ترین\" را بدهیم خروجی آن \"عالی\" خواهد بود اما اگر به تابع lemmatizer این عبارت را بدهیم، خودش را بر می گرداند. \n",
    "در نتیجه به وسیله ی این دو می توانیم کلماتی که دارای یک ریشه مشخص هستند را به عنوان همان ریشه تلقی کنیم و تعداد آن ها را دقیق تر داشته باشیم.\n",
    "\n",
    "فاز دوم: فرآیند مسئله\n",
    "\n",
    "همان طور که گفته شد برای تشخیص recommended یا not_recommended بودن می خواهیم از naïve bayes و از روش bag of words استفاده کنیم. در این روش به هر کلمه مستقل از جایگاهش نگاه می کنیم و feature  های مسئله را تعداد هر کلمه در کلاس مربوطه را در نظر می گیریم یعنی هرچه تعدا یک کلمه در یک کلاس بیش تر باشد با احتمال بیش تری به آن کلاس تعلق دارد. به این منظور از naïve bayes استفاده می کنیم  که فرمول آن طبق شکل در صورت پروژه موجود است. \n",
    " از آنجایی که می خواهیم احتمال recommend بودن یا not_recommended بودن را با داشتن متن(مجموعه ای از کلمات) و تیتر کامنت حدس بزنیم پس posterior probability ما به  این صورت خواهد بود. همان طور که در فرمول داریم می توانیم با محاسبه likelihood این محاسبات را انجام دهیم. که در این جا likelihood برای مسئله ما احتمال متن(تک تک کلمه های متن) به شرط recommended بودن یا not_recommended بودن می باشد و class prior probability  احتمال recommended و not_recommended می باشد و predictor prior probability  همان احتمال متن(مجموعه ای از کلمات می باشد) .\n",
    "همان طور که گفته شد likelihood برای مسئله ما احتمال متن(تک تک کلمه های متن) به شرط recommended بودن یا not_recommended بودن می باشد و از آن جایی که متن ما مجموعه ای از کلمات است برای محاسبه ای این احتمال باید احتمال تک تک کلمات متن را به ازای recommended بودن و not_recommended بودن را محاسبه کنیم و با ضرب کردن احتمال تک تک کلمه ها در این دو حالت ، حالتی را انتخاب می کنیم که احتمال بیش تری دارد(recommended بودن و not_recommended بودن). \n",
    "برای انجام محاسبات این مرحله از همان train_tokendict ای که ساخته ایم استفاده می کنیم.از آن جایی که در این دیکشنری برای هر سطر از فایل توکن های مربوط به تیتر و کامنت را (به صورت تلفیقی) و  recommended بودن و not_recommended بودن   را داریم پس به ازای هر کلمه داخل این دیکشنری اگر در کامنتی بود که recommended بود آن را به لیستی به نام all_recommended_words اضافه می کنیم و counter ای به نام recommend_counter را یک واحد زیاد می کنیم که این counter نشان دهنده ی تعداد کل کلمات موجود در recommended می باشد. هم چنین اگر این کلمه در کامنتی بود که not_recoomended بود آن را به لیستی به نام all_notrecommended_words  اضافه می کنیم و مقدار not_recommend_counter را یک واحد افزایش می دهیم.  در این مرحلهp_r  وp_n_r را که نشان دهنده احتمال recommended بودن یا نبودن را نمایش می دهند قابل محاسبه است.(همان class prior probability)\n",
    "با استفاده از CountFrequency روی هر دو لیست all_notrecommended_words و all_recommended_word می توانیم دیکشنری ای داشته باشیم که key های آن کلمات داخل آن لیست می باشد و value هر کدام ، میزان تکرار آن ها در لیست می باشد. و حالا با تقسیم کردن هریک از value  ها به تعداد کل کلمات موجود در هریک از این دیکشنری ها می توانیم احتمال تک تک کلمات به ازای recommended بودن یا نبودن را محاسبه کنیم. و آن ها را در دیکشنری های جدیدی با نام های p_recommended_word و p_not_recommended_wored ذخیره می کنیم.\n",
    "حال با ایجاد دیتافریم جدیدی که محتوای آن همان دیتا فریم قبلی (محتوای فایل test)است  به همراه یک ستون اضافه تر(‘predict’)  می خواهیم recommended  بودن یا نبودن هر یک از کامنت ها را در ستون predict پیش بینی کنیم. \n",
    "به ازای هر توکن ای که داخل کامنت یک سطر از فایل test قرار دارد آن توکن را در هر دو دیکشنری پیدا می کنیم و مقدار احتمال آن را در p_recommend و یا در p_not_recommend  که به ترتیب در ابتدا به p_r و p_n_r ست شده است.\n",
    "در آخر (بعد از بررسی آخرین توکن یک کامنت) دو مقدار p_recommend و p_not_recommend را باهم مقایسه کرده و بر اساس آن مقدار ستون predict را حدس می زنیم.\n",
    "\n",
    "\n",
    "بخش Additive Smoothing  :\n",
    "\n",
    "همان طور که در توضیحات پروژه آمده است ممکن است کلمه ای در recommended ها نیامده باشد و در not_recommended وجود داشته باشد و مدل ما در این حالت حدس میزند که پیش بینی ما باید not_recommended باشد که این درست نیست. چرا ؟ زیرا که اگر کلمه ای در recommended ها نیامده باشد پس احتمال آن کلمه به شرط recommended بودن که برابر است با تعداد دفعاتی که آن کلمه در recommended آمده تقسیم بر تعداد همه ی کلمات موجود در recommended ، برابر 0 خواهد شد و از آن جایی که احتمال های کلمات مختلف به شرط recommended بودن در هم ضرب می شود پس کل این احتمال برابر 0 می شود که قطعا از احتمال not recommended کم تر می شود پس مدل ما در اینگونه موارد همواره حدس میزند که به کلاس not recommended تعلق دارد. برای حل این مشکل از روش additive smoothing  استفاده می کنیم که در آن مثلا احتمال هر کلمه به شرط recommended بودن به جای این که با تقسیم تعداد دفعاتی که آن کلمه در recommended  ها آمده بر تعداد کل کلمات موجود در recommended ها از ضریب آلفایی استفاده می کنیم که با صورت جمع می شود و در مخرج در تعداد انواع کلمات موجود در recommende (به طور مثال) ضرب شه و با تعداد همه کلمات موجود در recommended جمع می شود.\n",
    "استفاده از این نوع احتمال باعث می شود اگر کلمه در یکی از recommended ها یا  not recommended   ها باشد و در دیگری نباشد یا در هر دو نباشد و کلمه ای جدید باشد برای آن احتمالی غیر از صفر در نظر گرفته شود و تشخیص ما دقیق تر شود. \n",
    "\n",
    "\n",
    "فاز سوم: ارزیابی\n",
    "\n",
    "برای آن که بتوانیم میزان تاثیر هر یک از عملیاتی که روی داده های فایل هایمان انجام می دهیم را بیابیم طبق تعریف های داده شده 4 پارامتر گفته شده را محاسبه می کنیم. هم چنین هر یک از این 4 پارامتر را برای 4 حالت انجام یا عدم انجام preproccessing و additive smoothing حساب می کنیم.\n",
    "برای محاسبه ی precision باید تعداد سطر هایی که recommended بودند و ما هم آن ها را recommended تشخیص داده ایم را بر تعداد سطر هایی که recommended تشخیص داده شده اند(چه درست و چه غلط) تقسیم کنیم و این باعث می شود که مقداری که به دست می آوریم از مقدار واقعی کم تر باشد.\n",
    "برای محاسبه ی recall  باید تعداد کامنت هایی را که به درستی recommended تشخیص داده شده اند را بر تعداد کامنت هایی که در مجموعه داده test recommended  بوده اند تقسیم کنیم که این باعث می شود مقدار به دست آمده خیلی خوشبینانه باشد و از مقدار واقعی بیش تر باشد.\n",
    "برای محاسبه ی F1  از میانگینی استفاده می شود که میانگین همساز (Harmonic Mean) است که به میانگین توافقی نیز معروف است. زمانی که مقادیر از نوع نرخ یا نسبت باشند، از این نوع میانگین برای محاسبه متوسط داده ها استفاده میشود. که در آن تعداد نرخ هایی را که داریم بر مجموع معکوس نرخ ها تقسیم می شود. و میانگین همساز همیشه از میانگین حسابی کمتر است و زمانی که نرخ برایمان اهمیت دارد و داده هایمان غیر صفر است    کاربرد دارد.\n",
    "طبق خروجی نمایش داده شده می توان مقدار 4 پارامتر خواسته شده برای 4 حالت مختلف گفته شده را مشاهده کرد.\n",
    "\n",
    "5 کامنتی که در مدل ما به اشتباه تشخیص داده شده اند در خروجی آمده است. \n",
    "یکی از مشکلاتی که ممکن است باعث شده باشد که اشتباه تشخیص داده باشیم این است که در صورت مساوی بودن احتمال ها در حالت recommend  و not recommended  در نظر گرفته ایم که آن کامنت را متعلق به کلاس recommended بداند در صورتی که ممکن است واقعا به کلاس not recommended  تعلق داشته باشد.هم چنین ممکن است کاربری نظر خود را به صورت دوپهلو و کنایه ای نوشته باشد و مثلا در آن از جملاتی مثل وااااقعن عالی بود...استفاده کند ولی not recommended باشد چون نظرش را به صورت کنایه آمیز مطرح کرده است و ما با وجود کلماتی مثل عالی آن را recommended حدس زده باشیم.\n",
    "\n",
    "تحلیل نمودار ها: همان طور که از نمودار ها پیداست هنگامی که precision , accuracy , f1 را محاسبه می کنیم به ترتیب مدل 1 بهتر از 2 بهتر از 3 بهتر از 4 می باشد و این بدان معناست که روی محاسبه ی اینگونه احتمال ها روش های پیش پردازش و additive smoothing تاثیر دارد اما در محاسبه ی recall  این را نمی بینیم و در دوحالتی که additive smoothing نداریم این مقدار بیش تر است. \n",
    "همچنین در هر مدل می توان دید که مقدار accuracy و f1 خیلی به هم نزدیک اند ولی مقدار recall و  precision  با آن ها فاصله دارد و مقدار  precision از recall کم تر می باشد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preproccessing_and_aditive_smoothing : Accuracy: 0.925\n",
      "preproccessing_and_aditive_smoothing : precision: 0.9066985645933014\n",
      "preproccessing_and_aditive_smoothing : recall: 0.9475\n",
      "preproccessing_and_aditive_smoothing : f1: 0.9266503667481663\n",
      "\n",
      "\n",
      "without_preproccesing_and_with_additive_smoothing : Accuracy: 0.91125\n",
      "without_preproccesing_and_with_additive_smoothing : precision: 0.8870588235294118\n",
      "without_preproccesing_and_with_additive_smoothing : recall: 0.9425\n",
      "without_preproccesing_and_with_additive_smoothing : f1: 0.913939393939394\n",
      "\n",
      "\n",
      "with_preproccessing_and_without_additive_smoothing : Accuracy: 0.87625\n",
      "with_preproccessing_and_without_additive_smoothing : precision: 0.8115942028985508\n",
      "with_preproccessing_and_without_additive_smoothing : recall: 0.98\n",
      "with_preproccessing_and_without_additive_smoothing : f1: 0.8878822197055493\n",
      "\n",
      "\n",
      "without_preproccessing_and_additive_smoothing : Accuracy: 0.85625\n",
      "without_preproccessing_and_additive_smoothing : precision: 0.7844311377245509\n",
      "without_preproccessing_and_additive_smoothing : recall: 0.9825\n",
      "without_preproccessing_and_additive_smoothing : f1: 0.872364039955605\n",
      "\n",
      "\n",
      "     title                                            comment    recommend  \\\n",
      "0  وری گود  تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش  recommended   \n",
      "\n",
      "           predict  \n",
      "0  not_recommended  \n",
      "\n",
      "\n",
      "              title                                            comment  \\\n",
      "3  دستگاه خیلی ضعیف  من این فیس براس چند روز یپش به دستم رسید و الا...   \n",
      "\n",
      "         recommend      predict  \n",
      "3  not_recommended  recommended  \n",
      "\n",
      "\n",
      "                  title                                            comment  \\\n",
      "5  خوب ولی کارایی محدود  مدل 46MM به دست شما نخواهد رسید و به جای آن مد...   \n",
      "\n",
      "         recommend      predict  \n",
      "5  not_recommended  recommended  \n",
      "\n",
      "\n",
      "            title                                            comment  \\\n",
      "8  نقد پس از خرید  سلام ، راحت شدم از کابل شارژ ، توصیه میشود به ...   \n",
      "\n",
      "     recommend          predict  \n",
      "8  recommended  not_recommended  \n",
      "\n",
      "\n",
      "            title                                            comment  \\\n",
      "19  خیالم راحت شد  فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر م...   \n",
      "\n",
      "      recommend          predict  \n",
      "19  recommended  not_recommended  \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "def delet_stop_words(mylist):\n",
    "    for i in mylist:\n",
    "        if(len(i)<=2):\n",
    "            if(i==\"بد\" or i==\"کم\"):\n",
    "                continue\n",
    "            else:\n",
    "                mylist.remove(i)\n",
    "    return mylist\n",
    "\n",
    "def CountFrequency(my_list):\n",
    "    count = {} \n",
    "    for i in my_list: \n",
    "        count[i] = count.get(i, 0) + 1\n",
    "    return count\n",
    "\n",
    "def preproccessing_and_aditive_smoothing():\n",
    "    data1 = pd.read_csv('comment_train.csv')   \n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    train_tokendict={}\n",
    "    test_tokendict={}\n",
    "    k=0\n",
    "    for row, column in data1.iterrows():\n",
    "        train_tokenlist=[]\n",
    "        n=normalizer.normalize(column[0])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            train_tokenlist.append(s)\n",
    "        n=normalizer.normalize(column[1])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            train_tokenlist.append(s)\n",
    "        train_tokenlist.append(column[2])\n",
    "        train_tokendict[k]=train_tokenlist   \n",
    "        k=k+1\n",
    "    recommend_counter=0\n",
    "    not_recommend_counter=0\n",
    "    all_recommended_words=[]\n",
    "    all_notrecommended_words=[]\n",
    "    for i in range(0,len(train_tokendict)):\n",
    "        l=len(train_tokendict[i])\n",
    "        if(train_tokendict[i][l-1]==\"recommended\"):\n",
    "            recommend_counter=recommend_counter+1\n",
    "            all_recommended_words.extend(train_tokendict[i][:-1])\n",
    "        if(train_tokendict[i][l-1]==\"not_recommended\"):\n",
    "            not_recommend_counter=not_recommend_counter+1\n",
    "            all_notrecommended_words.extend(train_tokendict[i][:-1])\n",
    "    sum1=recommend_counter+not_recommend_counter\n",
    "    p_r=recommend_counter/sum1\n",
    "    p_n_r=not_recommend_counter/sum1\n",
    "    c1=CountFrequency(all_recommended_words)\n",
    "    c2=CountFrequency(all_notrecommended_words)\n",
    "    total1 = sum(iter(c1.values()), 0.0)\n",
    "    total2=sum(iter(c2.values()), 0.0)\n",
    "    p_recommended_word={k: v / total1 for k, v in iter(c1.items())}\n",
    "    p_not_recommended_wored={k: v / total2 for k, v in iter(c2.items())}\n",
    "    a=0.001\n",
    "    AS_p_recommended_word={k: (v+a) / (total1+a*len(c1.keys())) for k, v in iter(c1.items())}\n",
    "    AS_p_not_recommended_wored={k: (v+a) / (total2+a*len(c2.keys())) for k, v in iter(c2.items())}\n",
    "    data2 = pd.read_csv('comment_test.csv')\n",
    "    df2 = pd.DataFrame(data2, columns= ['title','comment','recommend','predict'])\n",
    "    k=0\n",
    "    predict_list=[]\n",
    "    for row, column in df2.iterrows():\n",
    "        test_tokenlist=[]\n",
    "        n=normalizer.normalize(column[0])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            test_tokenlist.append(s)\n",
    "        n=normalizer.normalize(column[1])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            test_tokenlist.append(s)\n",
    "        p_recommend=p_r\n",
    "        p_not_recommend=p_n_r\n",
    "        for i in test_tokenlist:\n",
    "            if i in AS_p_recommended_word:\n",
    "                p_recommend=p_recommend*(AS_p_recommended_word.get(i))\n",
    "            else:\n",
    "                p_recommend=p_recommend*(a/(total1+a*len(c1.keys())))\n",
    "            if i in AS_p_not_recommended_wored:\n",
    "                p_not_recommend=p_not_recommend*(AS_p_not_recommended_wored.get(i))\n",
    "            else:\n",
    "                p_not_recommend=p_not_recommend*(a/(total2+a*len(c2.keys())))\n",
    "        if(p_recommend >= p_not_recommend):\n",
    "            predict_list.append('recommended')\n",
    "        if(p_recommend < p_not_recommend):\n",
    "            predict_list.append('not_recommended')\n",
    "    df2['predict']=predict_list\n",
    "    correct_detect_counter=0\n",
    "    correct_detect_recommended_counter=0\n",
    "    all_recommended_detect=0\n",
    "    total_recommended_counter=0\n",
    "    for row, column in df2.iterrows():\n",
    "        if(column[2]==column[3]):\n",
    "            correct_detect_counter=correct_detect_counter+1\n",
    "            if(column[2]=='recommended'):\n",
    "                correct_detect_recommended_counter=correct_detect_recommended_counter+1\n",
    "        if(column[3]=='recommended'):\n",
    "            all_recommended_detect=all_recommended_detect+1\n",
    "        if(column[2]=='recommended'):\n",
    "            total_recommended_counter=total_recommended_counter+1\n",
    "    count_row = df2.shape[0]\n",
    "    Accuracy=correct_detect_counter/count_row \n",
    "    precision=correct_detect_recommended_counter/all_recommended_detect\n",
    "    recall=correct_detect_recommended_counter/total_recommended_counter\n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    print(\"preproccessing_and_aditive_smoothing : Accuracy:\",Accuracy)\n",
    "    print(\"preproccessing_and_aditive_smoothing : precision:\",precision)\n",
    "    print(\"preproccessing_and_aditive_smoothing : recall:\",recall)\n",
    "    print(\"preproccessing_and_aditive_smoothing : f1:\",f1)\n",
    "    return df2,Accuracy,precision,recall,f1\n",
    "\n",
    "def without_preproccesing_and_with_additive_smoothing():\n",
    "    data3 = pd.read_csv('comment_train.csv')   \n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    train_tokendict2={}\n",
    "    test_tokendict2={}\n",
    "    k=0\n",
    "    for row, column in data3.iterrows():\n",
    "        train_tokenlist2=[]\n",
    "        t=word_tokenize(column[0])\n",
    "        train_tokenlist2.extend(t)\n",
    "        t=word_tokenize(column[1])\n",
    "        train_tokenlist2.extend(t)\n",
    "        train_tokenlist2.append(column[2])\n",
    "        train_tokendict2[k]=train_tokenlist2   \n",
    "        k=k+1\n",
    "    recommend_counter=0\n",
    "    not_recommend_counter=0\n",
    "    all_recommended_words2=[]\n",
    "    all_notrecommended_words2=[]\n",
    "    for i in range(0,len(train_tokendict2)):\n",
    "        l=len(train_tokendict2[i])\n",
    "        if(train_tokendict2[i][l-1]==\"recommended\"):\n",
    "            recommend_counter=recommend_counter+1\n",
    "            all_recommended_words2.extend(train_tokendict2[i][:-1])\n",
    "        if(train_tokendict2[i][l-1]==\"not_recommended\"):\n",
    "            not_recommend_counter=not_recommend_counter+1\n",
    "            all_notrecommended_words2.extend(train_tokendict2[i][:-1])\n",
    "    sum1=recommend_counter+not_recommend_counter\n",
    "    p_r2=recommend_counter/sum1\n",
    "    p_n_r2=not_recommend_counter/sum1\n",
    "    c1_2=CountFrequency(all_recommended_words2)\n",
    "    c2_2=CountFrequency(all_notrecommended_words2)\n",
    "    total1 = sum(iter(c1_2.values()), 0.0)\n",
    "    total2=sum(iter(c2_2.values()), 0.0)\n",
    "    p_recommended_word2={k: v / total1 for k, v in iter(c1_2.items())}\n",
    "    p_not_recommended_wored2={k: v / total2 for k, v in iter(c2_2.items())}\n",
    "    a=0.001\n",
    "    AS_p_recommended_word2={k: (v+a) / (total1+a*len(c1_2.keys())) for k, v in iter(c1_2.items())}\n",
    "    AS_p_not_recommended_wored2={k: (v+a) / (total2+a*len(c2_2.keys())) for k, v in iter(c2_2.items())}\n",
    "    data4 = pd.read_csv('comment_test.csv')\n",
    "    df3 = pd.DataFrame(data4, columns= ['title','comment','recommend','predict'])\n",
    "    k=0\n",
    "    predict_list2=[]\n",
    "    for row, column in df3.iterrows():\n",
    "        test_tokenlist2=[]\n",
    "        t=word_tokenize(column[0])\n",
    "        test_tokenlist2.extend(t)\n",
    "        t=word_tokenize(column[1])\n",
    "        test_tokenlist2.extend(t)\n",
    "        p_recommend=p_r2\n",
    "        p_not_recommend=p_n_r2\n",
    "        for i in test_tokenlist2:\n",
    "            if i in AS_p_recommended_word2:\n",
    "                p_recommend=p_recommend*(AS_p_recommended_word2.get(i))\n",
    "            else:\n",
    "                p_recommend=p_recommend*(a/(total1+a*len(c1_2.keys())))\n",
    "            if i in AS_p_not_recommended_wored2:\n",
    "                p_not_recommend=p_not_recommend*(AS_p_not_recommended_wored2.get(i))\n",
    "            else:\n",
    "                p_not_recommend=p_not_recommend*(a/(total2+a*len(c2_2.keys())))\n",
    "        if(p_recommend >= p_not_recommend):\n",
    "            predict_list2.append('recommended')\n",
    "        if(p_recommend < p_not_recommend):\n",
    "            predict_list2.append('not_recommended')\n",
    "    df3['predict']=predict_list2\n",
    "    correct_detect_counter=0\n",
    "    correct_detect_recommended_counter=0\n",
    "    all_recommended_detect=0\n",
    "    total_recommended_counter=0\n",
    "    for row, column in df3.iterrows():\n",
    "        if(column[2]==column[3]):\n",
    "            correct_detect_counter=correct_detect_counter+1\n",
    "            if(column[2]=='recommended'):\n",
    "                correct_detect_recommended_counter=correct_detect_recommended_counter+1\n",
    "        if(column[3]=='recommended'):\n",
    "            all_recommended_detect=all_recommended_detect+1\n",
    "        if(column[2]=='recommended'):\n",
    "            total_recommended_counter=total_recommended_counter+1\n",
    "    count_row = df3.shape[0]\n",
    "    Accuracy=correct_detect_counter/count_row \n",
    "    precision=correct_detect_recommended_counter/all_recommended_detect\n",
    "    recall=correct_detect_recommended_counter/total_recommended_counter\n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    print(\"without_preproccesing_and_with_additive_smoothing : Accuracy:\",Accuracy)\n",
    "    print(\"without_preproccesing_and_with_additive_smoothing : precision:\",precision)\n",
    "    print(\"without_preproccesing_and_with_additive_smoothing : recall:\",recall)\n",
    "    print(\"without_preproccesing_and_with_additive_smoothing : f1:\",f1)\n",
    "    return Accuracy,precision,recall,f1\n",
    "def with_preproccessing_and_without_additive_smoothing():\n",
    "    data5 = pd.read_csv('comment_train.csv')   \n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    train_tokendict={}\n",
    "    test_tokendict={}\n",
    "    k=0\n",
    "    for row, column in data5.iterrows():\n",
    "        train_tokenlist=[]\n",
    "        n=normalizer.normalize(column[0])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            train_tokenlist.append(s)\n",
    "        n=normalizer.normalize(column[1])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            train_tokenlist.append(s)\n",
    "        train_tokenlist.append(column[2])\n",
    "        train_tokendict[k]=train_tokenlist   \n",
    "        k=k+1\n",
    "    recommend_counter=0\n",
    "    not_recommend_counter=0\n",
    "    all_recommended_words=[]\n",
    "    all_notrecommended_words=[]\n",
    "    for i in range(0,len(train_tokendict)):\n",
    "        l=len(train_tokendict[i])\n",
    "        if(train_tokendict[i][l-1]==\"recommended\"):\n",
    "            recommend_counter=recommend_counter+1\n",
    "            all_recommended_words.extend(train_tokendict[i][:-1])\n",
    "        if(train_tokendict[i][l-1]==\"not_recommended\"):\n",
    "            not_recommend_counter=not_recommend_counter+1\n",
    "            all_notrecommended_words.extend(train_tokendict[i][:-1])\n",
    "    sum1=recommend_counter+not_recommend_counter\n",
    "    p_r=recommend_counter/sum1\n",
    "    p_n_r=not_recommend_counter/sum1\n",
    "    c1=CountFrequency(all_recommended_words)\n",
    "    c2=CountFrequency(all_notrecommended_words)\n",
    "    total1 = sum(iter(c1.values()), 0.0)\n",
    "    total2=sum(iter(c2.values()), 0.0)\n",
    "    p_recommended_word={k: v / total1 for k, v in iter(c1.items())}\n",
    "    p_not_recommended_wored={k: v / total2 for k, v in iter(c2.items())}\n",
    "    data6 = pd.read_csv('comment_test.csv')\n",
    "    df4 = pd.DataFrame(data6, columns= ['title','comment','recommend','predict'])\n",
    "    k=0\n",
    "    predict_list=[]\n",
    "    for row, column in df4.iterrows():\n",
    "        test_tokenlist=[]\n",
    "        n=normalizer.normalize(column[0])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            test_tokenlist.append(s)\n",
    "        n=normalizer.normalize(column[1])\n",
    "        t=word_tokenize(n)\n",
    "        new_t=delet_stop_words(t)\n",
    "        for i in new_t:\n",
    "            s=stemmer.stem(i)\n",
    "            test_tokenlist.append(s)\n",
    "        p_recommend=p_r\n",
    "        p_not_recommend=p_n_r\n",
    "        for i in test_tokenlist:\n",
    "            if i in p_recommended_word:\n",
    "                p_recommend=p_recommend*(p_recommended_word.get(i))\n",
    "            else:\n",
    "                p_recommend=0\n",
    "            if i in  p_not_recommended_wored:\n",
    "                p_not_recommend=p_not_recommend*( p_not_recommended_wored.get(i))\n",
    "            else:\n",
    "                p_not_recommend=0\n",
    "        if(p_recommend >= p_not_recommend):\n",
    "            predict_list.append('recommended')\n",
    "        if(p_recommend < p_not_recommend):\n",
    "            predict_list.append('not_recommended')\n",
    "    df4['predict']=predict_list\n",
    "    correct_detect_counter=0\n",
    "    correct_detect_recommended_counter=0\n",
    "    all_recommended_detect=0\n",
    "    total_recommended_counter=0\n",
    "    for row, column in df4.iterrows():\n",
    "        if(column[2]==column[3]):\n",
    "            correct_detect_counter=correct_detect_counter+1\n",
    "            if(column[2]=='recommended'):\n",
    "                correct_detect_recommended_counter=correct_detect_recommended_counter+1\n",
    "        if(column[3]=='recommended'):\n",
    "            all_recommended_detect=all_recommended_detect+1\n",
    "        if(column[2]=='recommended'):\n",
    "            total_recommended_counter=total_recommended_counter+1\n",
    "    count_row = df4.shape[0]\n",
    "    Accuracy=correct_detect_counter/count_row \n",
    "    precision=correct_detect_recommended_counter/all_recommended_detect\n",
    "    recall=correct_detect_recommended_counter/total_recommended_counter\n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    print(\"with_preproccessing_and_without_additive_smoothing : Accuracy:\",Accuracy)\n",
    "    print(\"with_preproccessing_and_without_additive_smoothing : precision:\",precision)\n",
    "    print(\"with_preproccessing_and_without_additive_smoothing : recall:\",recall)\n",
    "    print(\"with_preproccessing_and_without_additive_smoothing : f1:\",f1)\n",
    "    return Accuracy,precision,recall,f1\n",
    "\n",
    "def without_preproccessing_and_additive_smoothing():\n",
    "    data7 = pd.read_csv('comment_train.csv')   \n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    train_tokendict={}\n",
    "    test_tokendict={}\n",
    "    k=0\n",
    "    for row, column in data7.iterrows():\n",
    "        train_tokenlist=[]\n",
    "        t=word_tokenize(column[0])\n",
    "        train_tokenlist.extend(t)\n",
    "        t=word_tokenize(column[1])\n",
    "        train_tokenlist.extend(t)\n",
    "        train_tokenlist.append(column[2])\n",
    "        train_tokendict[k]=train_tokenlist  \n",
    "        k=k+1\n",
    "    recommend_counter=0\n",
    "    not_recommend_counter=0\n",
    "    all_recommended_words=[]\n",
    "    all_notrecommended_words=[]\n",
    "    for i in range(0,len(train_tokendict)):\n",
    "        l=len(train_tokendict[i])\n",
    "        if(train_tokendict[i][l-1]==\"recommended\"):\n",
    "            recommend_counter=recommend_counter+1\n",
    "            all_recommended_words.extend(train_tokendict[i][:-1])\n",
    "        if(train_tokendict[i][l-1]==\"not_recommended\"):\n",
    "            not_recommend_counter=not_recommend_counter+1\n",
    "            all_notrecommended_words.extend(train_tokendict[i][:-1])\n",
    "    sum1=recommend_counter+not_recommend_counter\n",
    "    p_r=recommend_counter/sum1\n",
    "    p_n_r=not_recommend_counter/sum1\n",
    "    c1=CountFrequency(all_recommended_words)\n",
    "    c2=CountFrequency(all_notrecommended_words)\n",
    "    total1 = sum(iter(c1.values()), 0.0)\n",
    "    total2=sum(iter(c2.values()), 0.0)\n",
    "    p_recommended_word={k: v / total1 for k, v in iter(c1.items())}\n",
    "    p_not_recommended_wored={k: v / total2 for k, v in iter(c2.items())}\n",
    "    data8 = pd.read_csv('comment_test.csv')\n",
    "    df5 = pd.DataFrame(data8, columns= ['title','comment','recommend','predict'])\n",
    "    k=0\n",
    "    predict_list=[]\n",
    "    for row, column in df5.iterrows():\n",
    "        test_tokenlist=[]\n",
    "        t=word_tokenize(column[0])\n",
    "        test_tokenlist.extend(t)\n",
    "        t=word_tokenize(column[1])\n",
    "        test_tokenlist.extend(t)\n",
    "        p_recommend=p_r\n",
    "        p_not_recommend=p_n_r\n",
    "        for i in test_tokenlist:\n",
    "            if i in p_recommended_word:\n",
    "                p_recommend=p_recommend*(p_recommended_word.get(i))\n",
    "            else:\n",
    "                p_recommend=0\n",
    "            if i in  p_not_recommended_wored:\n",
    "                p_not_recommend=p_not_recommend*( p_not_recommended_wored.get(i))\n",
    "            else:\n",
    "                p_not_recommend=0\n",
    "        if(p_recommend >= p_not_recommend):\n",
    "            predict_list.append('recommended')\n",
    "        if(p_recommend < p_not_recommend):\n",
    "            predict_list.append('not_recommended')\n",
    "    df5['predict']=predict_list\n",
    "    correct_detect_counter=0\n",
    "    correct_detect_recommended_counter=0\n",
    "    all_recommended_detect=0\n",
    "    total_recommended_counter=0\n",
    "    for row, column in df5.iterrows():\n",
    "        if(column[2]==column[3]):\n",
    "            correct_detect_counter=correct_detect_counter+1\n",
    "            if(column[2]=='recommended'):\n",
    "                correct_detect_recommended_counter=correct_detect_recommended_counter+1\n",
    "        if(column[3]=='recommended'):\n",
    "            all_recommended_detect=all_recommended_detect+1\n",
    "        if(column[2]=='recommended'):\n",
    "            total_recommended_counter=total_recommended_counter+1\n",
    "    count_row = df5.shape[0]\n",
    "    Accuracy=correct_detect_counter/count_row \n",
    "    precision=correct_detect_recommended_counter/all_recommended_detect\n",
    "    recall=correct_detect_recommended_counter/total_recommended_counter\n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    print(\"without_preproccessing_and_additive_smoothing : Accuracy:\",Accuracy)\n",
    "    print(\"without_preproccessing_and_additive_smoothing : precision:\",precision)\n",
    "    print(\"without_preproccessing_and_additive_smoothing : recall:\",recall)\n",
    "    print(\"without_preproccessing_and_additive_smoothing : f1:\",f1)\n",
    "    return Accuracy,precision,recall,f1\n",
    "    \n",
    "df,a1,p1,r1,f1_1=preproccessing_and_aditive_smoothing()\n",
    "print(\"\\n\")\n",
    "a2,p2,r2,f1_2=without_preproccesing_and_with_additive_smoothing()\n",
    "print(\"\\n\")\n",
    "a3,p3,r3,f1_3=with_preproccessing_and_without_additive_smoothing()\n",
    "print(\"\\n\")\n",
    "a4,p4,r4,f1_4=without_preproccessing_and_additive_smoothing()\n",
    "print(\"\\n\")\n",
    "print(df.loc[[0]])\n",
    "print(\"\\n\")\n",
    "print(df.loc[[3]])\n",
    "print(\"\\n\")\n",
    "print(df.loc[[5]])\n",
    "print(\"\\n\")\n",
    "print(df.loc[[8]])\n",
    "print(\"\\n\")\n",
    "print(df.loc[[19]])\n",
    "print(\"\\n\")\n",
    "\n",
    "y_list1=[]\n",
    "y_list1.append(a1)\n",
    "y_list1.append(a2)\n",
    "y_list1.append(a3)\n",
    "y_list1.append(a4)\n",
    "x_list1=['model1','model2','model3','model4'] \n",
    "plt.title(\"Accuracy\")\n",
    "plt.bar(x_list1, y_list1)  \n",
    "plt.show()\n",
    "\n",
    "y_list2=[]\n",
    "y_list2.append(p1)\n",
    "y_list2.append(p2)\n",
    "y_list2.append(p3)\n",
    "y_list2.append(p4)\n",
    "plt.title(\"precision\")\n",
    "plt.bar(x_list1, y_list2)  \n",
    "plt.show()\n",
    "\n",
    "y_list3=[]\n",
    "y_list3.append(r1)\n",
    "y_list3.append(r2)\n",
    "y_list3.append(r3)\n",
    "y_list3.append(r4)\n",
    "plt.title(\"recall\")\n",
    "plt.bar(x_list1, y_list3)  \n",
    "plt.show()\n",
    "\n",
    "y_list4=[]\n",
    "y_list4.append(f1_1)\n",
    "y_list4.append(f1_2)\n",
    "y_list4.append(f1_3)\n",
    "y_list4.append(f1_4)\n",
    "plt.title(\"F1\")\n",
    "plt.bar(x_list1, y_list4)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "direction": "ltr",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
